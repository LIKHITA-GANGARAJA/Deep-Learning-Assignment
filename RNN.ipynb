{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJTDshOd6YbW72MJJ6cuAI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"S0jZ3pQQtg1x"},"outputs":[],"source":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6e995c0c","executionInfo":{"status":"ok","timestamp":1763213722742,"user_tz":-330,"elapsed":1750,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"a268d200-d58b-442f-9dcf-5142092865b9"},"source":["import pandas as pd\n","\n","tweets_df = pd.read_csv('Tweets.csv')\n","print(\"Tweets DataFrame Head:\\n\", tweets_df.head())\n","print(\"\\nTweets DataFrame Columns:\\n\", tweets_df.columns)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Tweets DataFrame Head:\n","              tweet_id airline_sentiment  airline_sentiment_confidence  \\\n","0  570306133677760513           neutral                        1.0000   \n","1  570301130888122368          positive                        0.3486   \n","2  570301083672813571           neutral                        0.6837   \n","3  570301031407624196          negative                        1.0000   \n","4  570300817074462722          negative                        1.0000   \n","\n","  negativereason  negativereason_confidence         airline  \\\n","0            NaN                        NaN  Virgin America   \n","1            NaN                     0.0000  Virgin America   \n","2            NaN                        NaN  Virgin America   \n","3     Bad Flight                     0.7033  Virgin America   \n","4     Can't Tell                     1.0000  Virgin America   \n","\n","  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n","0                    NaN     cairdin                 NaN              0   \n","1                    NaN    jnardino                 NaN              0   \n","2                    NaN  yvonnalynn                 NaN              0   \n","3                    NaN    jnardino                 NaN              0   \n","4                    NaN    jnardino                 NaN              0   \n","\n","                                                text tweet_coord  \\\n","0                @VirginAmerica What @dhepburn said.         NaN   \n","1  @VirginAmerica plus you've added commercials t...         NaN   \n","2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n","3  @VirginAmerica it's really aggressive to blast...         NaN   \n","4  @VirginAmerica and it's a really big bad thing...         NaN   \n","\n","               tweet_created tweet_location               user_timezone  \n","0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n","1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n","2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n","3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n","4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n","\n","Tweets DataFrame Columns:\n"," Index(['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence',\n","       'negativereason', 'negativereason_confidence', 'airline',\n","       'airline_sentiment_gold', 'name', 'negativereason_gold',\n","       'retweet_count', 'text', 'tweet_coord', 'tweet_created',\n","       'tweet_location', 'user_timezone'],\n","      dtype='object')\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77b5b454","executionInfo":{"status":"ok","timestamp":1763213742739,"user_tz":-330,"elapsed":35,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"aa790b85-4a5b-49c0-9b8d-4772319155bc"},"source":["import sqlite3\n","import pandas as pd\n","\n","# Establish a connection to the SQLite database\n","conn = sqlite3.connect('database.sqlite')\n","\n","# Inspect the tables in the database\n","# This query will list all tables in the database\n","tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n","tables_df = pd.read_sql_query(tables_query, conn)\n","print(\"Tables in database.sqlite:\\n\", tables_df)\n","\n","# Based on the original reasoning, assuming there's a 'tweets' table.\n","# If the 'tweets' table exists, let's inspect its schema.\n","# If not, further inspection would be needed to find the relevant table.\n","if 'tweets' in tables_df['name'].values:\n","    print(\"\\nInspecting schema for 'tweets' table:\")\n","    schema_query = \"PRAGMA table_info(tweets);\"\n","    schema_df = pd.read_sql_query(schema_query, conn)\n","    print(schema_df)\n","\n","    # Based on the typical naming conventions and the 'Tweets.csv' file,\n","    # I'll look for columns that could represent text and sentiment.\n","    # If 'text' and 'sentiment' are not found, we will need to deduce them.\n","    # For this attempt, I will try to use `text` and `airline_sentiment` if they exist,\n","    # or infer similar columns from the schema.\n","    # Let's assume the table has columns like 'text' and 'sentiment_label' or similar.\n","    # Given the previous error and common sentiment datasets, a column like 'sentiment'\n","    # or 'label' might exist. If not, I'll need to infer from the output.\n","\n","    # For demonstration, let's assume columns are 'text' and 'sentiment' (if they existed).\n","    # Since 'sentiment' caused an error, I will first check the schema and then adjust.\n","    # Temporarily, I'll query for all columns in 'tweets' table to see what's available.\n","    # A more robust solution would dynamically check for `text` and a sentiment-like column.\n","\n","    # Let's try to fetch a limited number of rows and all columns to infer names.\n","    # This is a common debugging step when column names are unknown.\n","    try:\n","        all_cols_query = \"SELECT * FROM tweets LIMIT 5;\"\n","        sample_df = pd.read_sql_query(all_cols_query, conn)\n","        print(\"\\nSample data from 'tweets' table (first 5 rows, all columns):\\n\", sample_df)\n","        print(\"\\nColumns in 'tweets' table:\\n\", sample_df.columns.tolist())\n","\n","        # After inspecting sample_df.columns, we can refine the query.\n","        # If 'text' exists and 'sentiment' doesn't, we need to find the sentiment equivalent.\n","        # For this example, let's assume after inspecting 'sample_df.columns' we find 'tweet_text' and 'sentiment_label'.\n","        # Or, if the table structure is similar to Tweets.csv, it might have 'text' and 'airline_sentiment_label'.\n","        # Given the task is about sentiment analysis and `Tweets.csv` has `airline_sentiment` and `text`,\n","        # a plausible alternative in `database.sqlite` could be similar names.\n","\n","        # For now, let's re-attempt the query with just `text` and see if it exists.\n","        # If `text` exists and `sentiment` doesn't, we will need to list tables and columns to find the correct sentiment column.\n","\n","        # To fix the 'no such column: sentiment' error, we need to check available columns.\n","        # If the 'tweets' table exists, let's try to query all columns first to understand its structure.\n","        # I'll create a new query that is more likely to work after this inspection step.\n","        # For now, I'll just print the schema and a sample of the data to inform the next step.\n","\n","    except Exception as e:\n","        print(f\"Could not query 'tweets' table for all columns: {e}\")\n","\n","# Close the database connection (important to do this even if there's an error)\n","conn.close()\n","\n","print(\"Database connection closed.\")"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Tables in database.sqlite:\n","      name\n","0  Tweets\n","Database connection closed.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f16b5b0f","executionInfo":{"status":"ok","timestamp":1763213754753,"user_tz":-330,"elapsed":46,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"c655347a-8185-4c5f-d9af-feb466ada7ec"},"source":["import sqlite3\n","import pandas as pd\n","\n","# Establish a connection to the SQLite database\n","conn = sqlite3.connect('database.sqlite')\n","\n","# Inspect the tables in the database (already done, but re-confirming for correctness)\n","tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n","tables_df = pd.read_sql_query(tables_query, conn)\n","print(\"Tables in database.sqlite:\\n\", tables_df)\n","\n","# Correcting the table name to 'Tweets' (capital T) for schema inspection\n","if 'Tweets' in tables_df['name'].values:\n","    print(\"\\nInspecting schema for 'Tweets' table:\")\n","    schema_query = \"PRAGMA table_info(Tweets);\"\n","    schema_df = pd.read_sql_query(schema_query, conn)\n","    print(schema_df)\n","\n","    # Fetch a limited number of rows and all columns from 'Tweets' table to infer names.\n","    try:\n","        all_cols_query = \"SELECT * FROM Tweets LIMIT 5;\"\n","        sample_df = pd.read_sql_query(all_cols_query, conn)\n","        print(\"\\nSample data from 'Tweets' table (first 5 rows, all columns):\\n\", sample_df)\n","        print(\"\\nColumns in 'Tweets' table:\\n\", sample_df.columns.tolist())\n","\n","        # After inspecting the sample_df.columns, we can refine the query.\n","        # Given the previous `Tweets.csv` had 'text' and 'airline_sentiment',\n","        # we will look for similar columns here. Let's assume based on common\n","        # dataset structures and the CSV file, 'text' and 'sentiment' or\n","        # 'airline_sentiment' are likely candidates.\n","\n","        # Let's try to query with 'text' and 'airline_sentiment' now that we know\n","        # the table name is 'Tweets' and suspect these column names.\n","        # This is a refined step after knowing the table name and expecting\n","        # column names similar to the CSV.\n","\n","        # If the above inspection shows 'text' and 'airline_sentiment' columns,\n","        # then the next step will use them.\n","        # For now, let's just proceed with the inspection and then use the findings.\n","\n","    except Exception as e:\n","        print(f\"Could not query 'Tweets' table for all columns: {e}\")\n","else:\n","    print(\"Table 'Tweets' not found in database.sqlite.\")\n","\n","# Close the database connection\n","conn.close()\n","\n","print(\"Database connection closed.\")"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Tables in database.sqlite:\n","      name\n","0  Tweets\n","\n","Inspecting schema for 'Tweets' table:\n","    cid                          name     type  notnull dflt_value  pk\n","0     0                      tweet_id  INTEGER        0       None   1\n","1     1             airline_sentiment     TEXT        0       None   0\n","2     2  airline_sentiment_confidence  NUMERIC        0       None   0\n","3     3                negativereason     TEXT        0       None   0\n","4     4     negativereason_confidence  NUMERIC        0       None   0\n","5     5                       airline     TEXT        0       None   0\n","6     6        airline_sentiment_gold     TEXT        0       None   0\n","7     7                          name     TEXT        0       None   0\n","8     8           negativereason_gold     TEXT        0       None   0\n","9     9                 retweet_count  INTEGER        0       None   0\n","10   10                          text     TEXT        0       None   0\n","11   11                   tweet_coord     TEXT        0       None   0\n","12   12                 tweet_created     TEXT        0       None   0\n","13   13                tweet_location     TEXT        0       None   0\n","14   14                 user_timezone     TEXT        0       None   0\n","\n","Sample data from 'Tweets' table (first 5 rows, all columns):\n","              tweet_id airline_sentiment  airline_sentiment_confidence  \\\n","0  567588278875213824           neutral                             1   \n","1  567590027375702016          negative                             1   \n","2  567591480085463040          negative                             1   \n","3  567592368451248130          negative                             1   \n","4  567594449874587648          negative                             1   \n","\n","           negativereason negativereason_confidence    airline  \\\n","0                                                        Delta   \n","1              Can't Tell                    0.6503      Delta   \n","2             Late Flight                     0.346     United   \n","3             Late Flight                         1     United   \n","4  Customer Service Issue                    0.3451  Southwest   \n","\n","  airline_sentiment_gold         name negativereason_gold  retweet_count  \\\n","0                         JetBlueNews                                  0   \n","1                           nesi_1992                                  0   \n","2                           CPoutloud                                  0   \n","3                            brenduch                                  0   \n","4                            VahidESQ                                  0   \n","\n","                                                text tweet_coord  \\\n","0  @JetBlue's new CEO seeks the right balance to ...               \n","1  @JetBlue is REALLY getting on my nerves !! ğŸ˜¡ğŸ˜¡ ...               \n","2  @united yes. We waited in line for almost an h...               \n","3  @united the we got into the gate at IAH on tim...               \n","4  @SouthwestAir its cool that my bags take a bit...               \n","\n","               tweet_created   tweet_location               user_timezone  \n","0  2015-02-16 23:36:05 -0800              USA                      Sydney  \n","1  2015-02-16 23:43:02 -0800        undecided  Pacific Time (US & Canada)  \n","2  2015-02-16 23:48:48 -0800   Washington, DC                              \n","3  2015-02-16 23:52:20 -0800                                 Buenos Aires  \n","4  2015-02-17 00:00:36 -0800  Los Angeles, CA  Pacific Time (US & Canada)  \n","\n","Columns in 'Tweets' table:\n"," ['tweet_id', 'airline_sentiment', 'airline_sentiment_confidence', 'negativereason', 'negativereason_confidence', 'airline', 'airline_sentiment_gold', 'name', 'negativereason_gold', 'retweet_count', 'text', 'tweet_coord', 'tweet_created', 'tweet_location', 'user_timezone']\n","Database connection closed.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"549d04fc","executionInfo":{"status":"ok","timestamp":1763213762737,"user_tz":-330,"elapsed":40,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"aaa5786e-a756-44c0-f265-081a87fdbf72"},"source":["import sqlite3\n","import pandas as pd\n","\n","# Establish a connection to the SQLite database\n","conn = sqlite3.connect('database.sqlite')\n","\n","# Formulate the SQL query with the correct table and column names\n","sql_query = \"SELECT text, airline_sentiment FROM Tweets\"\n","\n","# Load the data from the SQLite database into a pandas DataFrame\n","sqlite_df = pd.read_sql_query(sql_query, conn)\n","\n","# Close the database connection\n","conn.close()\n","\n","print(\"SQLite DataFrame Head:\\n\", sqlite_df.head())\n","print(\"\\nSQLite DataFrame Columns:\\n\", sqlite_df.columns)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["SQLite DataFrame Head:\n","                                                 text airline_sentiment\n","0  @JetBlue's new CEO seeks the right balance to ...           neutral\n","1  @JetBlue is REALLY getting on my nerves !! ğŸ˜¡ğŸ˜¡ ...          negative\n","2  @united yes. We waited in line for almost an h...          negative\n","3  @united the we got into the gate at IAH on tim...          negative\n","4  @SouthwestAir its cool that my bags take a bit...          negative\n","\n","SQLite DataFrame Columns:\n"," Index(['text', 'airline_sentiment'], dtype='object')\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8db81693","executionInfo":{"status":"ok","timestamp":1763213770764,"user_tz":-330,"elapsed":53,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"ba80bf67-8486-45e6-f3f5-15cf94cc00f5"},"source":["combined_df = pd.concat([tweets_df[['text', 'airline_sentiment']], sqlite_df[['text', 'airline_sentiment']]], ignore_index=True)\n","\n","print(\"Combined DataFrame Head:\\n\", combined_df.head())\n","print(\"\\nCombined DataFrame Info:\\n\")\n","combined_df.info()\n","print(\"\\nCombined DataFrame Value Counts for airline_sentiment:\\n\", combined_df['airline_sentiment'].value_counts())"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined DataFrame Head:\n","                                                 text airline_sentiment\n","0                @VirginAmerica What @dhepburn said.           neutral\n","1  @VirginAmerica plus you've added commercials t...          positive\n","2  @VirginAmerica I didn't today... Must mean I n...           neutral\n","3  @VirginAmerica it's really aggressive to blast...          negative\n","4  @VirginAmerica and it's a really big bad thing...          negative\n","\n","Combined DataFrame Info:\n","\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 29125 entries, 0 to 29124\n","Data columns (total 2 columns):\n"," #   Column             Non-Null Count  Dtype \n","---  ------             --------------  ----- \n"," 0   text               29125 non-null  object\n"," 1   airline_sentiment  29125 non-null  object\n","dtypes: object(2)\n","memory usage: 455.2+ KB\n","\n","Combined DataFrame Value Counts for airline_sentiment:\n"," airline_sentiment\n","negative    18260\n","neutral      6168\n","positive     4697\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6c22e9ba","executionInfo":{"status":"ok","timestamp":1763213847980,"user_tz":-330,"elapsed":5902,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"993c5871-ec28-4ad7-914e-c47136c7c134"},"source":["import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string # Import the string module\n","\n","# Download necessary NLTK data (if not already downloaded)\n","try:\n","    nltk.data.find('corpora/stopwords')\n","except LookupError:\n","    nltk.download('stopwords')\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","try:\n","    nltk.data.find('tokenizers/punkt_tab')\n","except LookupError:\n","    nltk.download('punkt_tab')\n","\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_text(text):\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    # Remove URLs\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n","\n","    # Remove mentions (@username)\n","    text = re.sub(r'@\\w+', '', text)\n","\n","    # Remove hashtags (#topic)\n","    text = re.sub(r'#\\w+', '', text)\n","\n","    # Remove punctuation using string.punctuation\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","    # Remove numbers\n","    text = re.sub(r'\\d+', '', text)\n","\n","    # Remove leading/trailing whitespace\n","    text = text.strip()\n","\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Remove stopwords and non-alphabetic tokens\n","    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n","\n","    # Join the processed words back into a single string\n","    return \" \".join(filtered_tokens)\n","\n","print(\"Preprocessing function defined and NLTK data downloaded.\")\n","\n","# Apply the preprocessing function to the 'text' column\n","combined_df['cleaned_text'] = combined_df['text'].apply(preprocess_text)\n","\n","# Display the first few rows of combined_df with the new 'cleaned_text' column\n","print(\"\\nCombined DataFrame Head with cleaned_text:\\n\", combined_df[['text', 'cleaned_text']].head())"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Preprocessing function defined and NLTK data downloaded.\n","\n","Combined DataFrame Head with cleaned_text:\n","                                                 text  \\\n","0                @VirginAmerica What @dhepburn said.   \n","1  @VirginAmerica plus you've added commercials t...   \n","2  @VirginAmerica I didn't today... Must mean I n...   \n","3  @VirginAmerica it's really aggressive to blast...   \n","4  @VirginAmerica and it's a really big bad thing...   \n","\n","                                        cleaned_text  \n","0                                               said  \n","1      plus youve added commercials experience tacky  \n","2       didnt today must mean need take another trip  \n","3  really aggressive blast obnoxious entertainmen...  \n","4                               really big bad thing  \n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aa656c8","executionInfo":{"status":"ok","timestamp":1763214020547,"user_tz":-330,"elapsed":65,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"f4414792-b4f1-4373-c9a3-424d1ff497c7"},"source":["from sklearn.preprocessing import LabelEncoder\n","\n","# Initialize LabelEncoder\n","label_encoder = LabelEncoder()\n","\n","# Encode the 'airline_sentiment' column\n","combined_df['sentiment_label'] = label_encoder.fit_transform(combined_df['airline_sentiment'])\n","\n","# Display the mapping of labels to numbers (optional, but good for verification)\n","print(\"Sentiment Label Mapping:\")\n","for i, label in enumerate(label_encoder.classes_):\n","    print(f\"{label}: {i}\")\n","\n","# Display value counts of the new 'sentiment_label' column\n","print(\"\\nValue Counts for sentiment_label:\\n\", combined_df['sentiment_label'].value_counts())"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentiment Label Mapping:\n","negative: 0\n","neutral: 1\n","positive: 2\n","\n","Value Counts for sentiment_label:\n"," sentiment_label\n","0    18260\n","1     6168\n","2     4697\n","Name: count, dtype: int64\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7dde3b57","executionInfo":{"status":"ok","timestamp":1763214036083,"user_tz":-330,"elapsed":7774,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"e0027dbb-3f21-4472-8a72-5af59bdd8289"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Initialize a Keras Tokenizer\n","# num_words can be set based on the desired vocabulary size\n","# For now, let's pick a reasonable number like 10000, which covers most common words.\n","num_words = 10000\n","tokenizer = Tokenizer(num_words=num_words, oov_token=\"<unk>\")\n","\n","# Fit the tokenizer on the cleaned_text column to build the vocabulary\n","tokenizer.fit_on_texts(combined_df['cleaned_text'])\n","\n","print(f\"Tokenizer initialized with a vocabulary size of {num_words}.\")\n","print(f\"Number of unique words found: {len(tokenizer.word_index)}\")\n","print(\"First 10 words in the vocabulary:\\n\", list(tokenizer.word_index.items())[:10])"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizer initialized with a vocabulary size of 10000.\n","Number of unique words found: 11032\n","First 10 words in the vocabulary:\n"," [('<unk>', 1), ('flight', 2), ('get', 3), ('thanks', 4), ('cancelled', 5), ('service', 6), ('help', 7), ('time', 8), ('customer', 9), ('im', 10)]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"507cf919","executionInfo":{"status":"ok","timestamp":1763214054587,"user_tz":-330,"elapsed":2354,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"3a260517-20be-4f0d-e692-e87cc654050d"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Convert the cleaned text to sequences of integers\n","X_sequences = tokenizer.texts_to_sequences(combined_df['cleaned_text'])\n","\n","print(f\"First 5 text sequences (raw):\\n{X_sequences[:5]}\")\n","print(f\"Type of X_sequences: {type(X_sequences)}\")\n","print(f\"Length of X_sequences: {len(X_sequences)}\")"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["First 5 text sequences (raw):\n","[[128], [414, 426, 972, 2176, 111, 5393], [110, 36, 635, 440, 21, 74, 69, 107], [60, 3209, 3947, 3948, 836, 2717, 3210, 13, 376, 2391], [60, 355, 123, 371]]\n","Type of X_sequences: <class 'list'>\n","Length of X_sequences: 29125\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11b3a700","executionInfo":{"status":"ok","timestamp":1763214063944,"user_tz":-330,"elapsed":224,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"f23d78a1-6757-42ee-f96b-b7c4a192090b"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","# Determine an appropriate maximum sequence length (maxlen)\n","# Let's calculate the length of each sequence\n","sequence_lengths = [len(seq) for seq in X_sequences]\n","\n","# Calculate descriptive statistics for sequence lengths\n","print(\"Sequence Lengths Statistics:\")\n","print(f\"  Mean: {np.mean(sequence_lengths):.2f}\")\n","print(f\"  Median: {np.median(sequence_lengths)}\")\n","print(f\"  Max: {np.max(sequence_lengths)}\")\n","print(f\"  90th Percentile: {np.percentile(sequence_lengths, 90)}\")\n","print(f\"  95th Percentile: {np.percentile(sequence_lengths, 95)}\")\n","\n","# Choose maxlen based on a percentile to capture most of the data without excessively long sequences\n","# Let's use the 95th percentile as maxlen\n","maxlen = int(np.percentile(sequence_lengths, 95))\n","print(f\"\\nSelected maxlen for padding: {maxlen}\")\n","\n","# Pad the sequences to the determined maxlen\n","X_padded = pad_sequences(X_sequences, maxlen=maxlen, padding='post', truncating='post')\n","\n","# Print the shape of X_padded to verify\n","print(f\"\\nShape of X_padded: {X_padded.shape}\")"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Sequence Lengths Statistics:\n","  Mean: 8.64\n","  Median: 9.0\n","  Max: 21\n","  90th Percentile: 14.0\n","  95th Percentile: 15.0\n","\n","Selected maxlen for padding: 15\n","\n","Shape of X_padded: (29125, 15)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"61b96b42","executionInfo":{"status":"ok","timestamp":1763214092778,"user_tz":-330,"elapsed":68,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"ae9bed0f-f26b-4f3c-b4a1-f6d96a7808c2"},"source":["from sklearn.model_selection import train_test_split\n","\n","# Define the target variable\n","y = combined_df['sentiment_label']\n","\n","# Split the data into training and temporary sets (80% train, 20% temp)\n","X_train, X_temp, y_train, y_temp = train_test_split(X_padded, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Split the temporary set into validation and test sets (50% val, 50% test from temp)\n","X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n","\n","# Print the shapes of the resulting sets\n","print(f\"Shape of X_train: {X_train.shape}\")\n","print(f\"Shape of y_train: {y_train.shape}\")\n","print(f\"Shape of X_val: {X_val.shape}\")\n","print(f\"Shape of y_val: {y_val.shape}\")\n","print(f\"Shape of X_test: {X_test.shape}\")\n","print(f\"Shape of y_test: {y_test.shape}\")"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of X_train: (23300, 15)\n","Shape of y_train: (23300,)\n","Shape of X_val: (2912, 15)\n","Shape of y_val: (2912,)\n","Shape of X_test: (2913, 15)\n","Shape of y_test: (2913,)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"0e37c80f","executionInfo":{"status":"ok","timestamp":1763214109654,"user_tz":-330,"elapsed":151,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"d22f340c-2df7-4be9-833f-730b9817029c"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","# Define model parameters\n","vocab_size = num_words + 1  # num_words from tokenizer + 1 for padding/OOV token\n","embedding_dim = 128\n","output_dim = len(label_encoder.classes_) # Number of unique sentiment labels (3: negative, neutral, positive)\n","\n","# Initialize the Sequential model\n","model = Sequential()\n","\n","# Add Embedding layer\n","model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n","\n","# Add LSTM layer\n","model.add(LSTM(units=100))\n","\n","# Add Dense output layer for classification\n","model.add(Dense(units=output_dim, activation='softmax'))\n","\n","# Print the model summary to review the architecture\n","model.summary()\n"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"b9c2d398","executionInfo":{"status":"ok","timestamp":1763214118251,"user_tz":-330,"elapsed":71,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"df1c878c-3af4-4e8d-b966-7cf1fe2883da"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","# Define model parameters\n","vocab_size = num_words + 1  # num_words from tokenizer + 1 for padding/OOV token\n","embedding_dim = 128\n","output_dim = len(label_encoder.classes_) # Number of unique sentiment labels (3: negative, neutral, positive)\n","\n","# Initialize the Sequential model\n","model = Sequential()\n","\n","# Add Embedding layer (removed input_length as it's deprecated)\n","model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n","\n","# Add LSTM layer\n","model.add(LSTM(units=100))\n","\n","# Add Dense output layer for classification\n","model.add(Dense(units=output_dim, activation='softmax'))\n","\n","# Print the model summary to review the architecture\n","model.summary()\n"],"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"9798516c","executionInfo":{"status":"ok","timestamp":1763214129947,"user_tz":-330,"elapsed":237,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"e48229e7-3778-4cfb-a292-58a9639139e7"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","# Define model parameters\n","vocab_size = num_words + 1  # num_words from tokenizer + 1 for padding/OOV token\n","embedding_dim = 128\n","output_dim = len(label_encoder.classes_) # Number of unique sentiment labels (3: negative, neutral, positive)\n","\n","# Initialize the Sequential model\n","model = Sequential()\n","\n","# Add Embedding layer (removed input_length as it's deprecated in Keras 3 for this context)\n","model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n","\n","# Add LSTM layer\n","model.add(LSTM(units=100))\n","\n","# Add Dense output layer for classification\n","model.add(Dense(units=output_dim, activation='softmax'))\n","\n","# Explicitly build the model to see parameter counts in the summary\n","model.build(input_shape=(None, maxlen))\n","\n","# Print the model summary to review the architecture\n","model.summary()"],"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_2\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚     \u001b[38;5;34m1,280,128\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            â”‚        \u001b[38;5;34m91,600\u001b[0m â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              â”‚           \u001b[38;5;34m303\u001b[0m â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,128</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">91,600</span> â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">303</span> â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,372,031\u001b[0m (5.23 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,372,031</span> (5.23 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,372,031\u001b[0m (5.23 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,372,031</span> (5.23 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a71f9bb3","executionInfo":{"status":"ok","timestamp":1763214138897,"user_tz":-330,"elapsed":176,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"2fa8fa57-20b2-4c9d-a196-2ed4a7f10c93"},"source":["from tensorflow.keras.optimizers import Adam\n","\n","# Compile the model\n","model.compile(optimizer=Adam(learning_rate=0.001),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","print(\"Model compiled successfully with Adam optimizer, sparse_categorical_crossentropy loss, and accuracy metric.\")"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Model compiled successfully with Adam optimizer, sparse_categorical_crossentropy loss, and accuracy metric.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87217ed0","executionInfo":{"status":"ok","timestamp":1763214323123,"user_tz":-330,"elapsed":171872,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"5c66f5f8-1150-42a6-a801-6ad10185c198"},"source":["batch_size = 64\n","epochs = 10\n","\n","# Train the model\n","history = model.fit(X_train, y_train,\n","                    epochs=epochs,\n","                    batch_size=batch_size,\n","                    validation_data=(X_val, y_val))\n","\n","print(\"Model training complete.\")"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 43ms/step - accuracy: 0.6929 - loss: 0.7344 - val_accuracy: 0.8393 - val_loss: 0.4302\n","Epoch 2/10\n","\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 42ms/step - accuracy: 0.8777 - loss: 0.3396 - val_accuracy: 0.8719 - val_loss: 0.3601\n","Epoch 3/10\n","\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step - accuracy: 0.9134 - loss: 0.2519 - val_accuracy: 0.8973 - val_loss: 0.3199\n","Epoch 4/10\n","\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step - accuracy: 0.9424 - loss: 0.1763 - val_accuracy: 0.8959 - val_loss: 0.3288\n","Epoch 5/10\n","\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step - accuracy: 0.9522 - loss: 0.1443 - val_accuracy: 0.9121 - val_loss: 0.3349\n","Epoch 6/10\n","\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 43ms/step - accuracy: 0.9610 - loss: 0.1196 - val_accuracy: 0.9203 - val_loss: 0.3020\n","Epoch 7/10\n","\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.9658 - loss: 0.0990 - val_accuracy: 0.9231 - val_loss: 0.3067\n","Epoch 8/10\n","\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 47ms/step - accuracy: 0.9694 - loss: 0.0869 - val_accuracy: 0.9255 - val_loss: 0.3179\n","Epoch 9/10\n","\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 42ms/step - accuracy: 0.9746 - loss: 0.0688 - val_accuracy: 0.9245 - val_loss: 0.3411\n","Epoch 10/10\n","\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 42ms/step - accuracy: 0.9761 - loss: 0.0668 - val_accuracy: 0.9238 - val_loss: 0.3616\n","Model training complete.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06cf632f","executionInfo":{"status":"ok","timestamp":1763214439171,"user_tz":-330,"elapsed":1779,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"29f215f3-a4d4-4407-bd09-cb8a91d3804d"},"source":["loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n","\n","print(f\"Test Loss: {loss:.4f}\")\n","print(f\"Test Accuracy: {accuracy:.4f}\")"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.3755\n","Test Accuracy: 0.9097\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2a2ddc99","executionInfo":{"status":"ok","timestamp":1763214449907,"user_tz":-330,"elapsed":2054,"user":{"displayName":"Likhita RVITM","userId":"03674174211816644446"}},"outputId":"de3ed4aa-49ad-41bb-f1a7-282c849b3113"},"source":["import numpy as np\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","# Predict the sentiment labels for X_test\n","y_pred_prob = model.predict(X_test)\n","\n","# Convert predicted probabilities to class labels\n","y_pred = np.argmax(y_pred_prob, axis=1)\n","\n","# Generate and print the classification report\n","print(\"\\nClassification Report:\\n\")\n","print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n","\n","# Generate and print the confusion matrix\n","print(\"\\nConfusion Matrix:\\n\")\n","print(confusion_matrix(y_test, y_pred))"],"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m92/92\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","    negative       0.95      0.94      0.94      1826\n","     neutral       0.85      0.83      0.84       617\n","    positive       0.83      0.91      0.87       470\n","\n","    accuracy                           0.91      2913\n","   macro avg       0.88      0.89      0.88      2913\n","weighted avg       0.91      0.91      0.91      2913\n","\n","\n","Confusion Matrix:\n","\n","[[1710   71   45]\n"," [  65  511   41]\n"," [  20   21  429]]\n"]}]}]}